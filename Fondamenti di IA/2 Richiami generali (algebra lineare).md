
### <font color="#ffff00">Matrici vettoriali</font>
$$
\begin{bmatrix}  
v_{1} \\  
. \\
. \\
. \\
. \\
v_{n}
\end{bmatrix}
$$
Si utilizza questa rappresentazione al fine di poter dimostrare anche matrici che vanno oltre il bidimensionale

### <font color="#ffff00">Sistema di equazioni lineari</font>

#definizione <font color="#00b050">Sistema di equazioni lineari</font>
$$ \begin{cases} a_{11}\times x+\dots a_{1n}\times x_{n}=b_{1} &  \\ . &\\ . \\ . \\ a_{mn}+a_{mn} \times x_{n} =b_{n}\end{cases}$$

### <font color="#ffff00">Come possiamo rappresentare le matrici in modo sintetico?</font>

Tramite questa rappresentazione:

$$A\in R^{m\times n}$$ dove $m$ sono le righe e $n$ sono le colonne

$$
\begin{bmatrix}  
a_{11} &.&.&.&a_{1n}\\  
. &.&.&.&.\\
. &.&.&.&.\\
. &.&.&.&.\\
. &.&.&.&.\\
v_{m_{1}}&.&.&.&a_{mn}
\end{bmatrix}
$$
Osserviamo che:

$$A+B=\{ a_{ij}+b_{ij} \}(i,j)\in[m]\times[n]$$
### <font color="#ffff00">Matrice di Hadmard</font>


### Matrice identit√†


### Matrice trasposta


### Inversa di Moore-Penrose


### Spazio vettoriale


### Sottospazio


### Base spazi vettoriali


### One hot encoding


### Trasformazioni lineari


### Norma

- #### Norma 1

- #### Norma 2

- #### Norma 0

- #### Norma $\infty$ 

- #### Norma $p$ 


### Prodotto interno


### Matrice simmetrica & Definitivamente positiva


### Prodotto scalare


### Spazio vettoriale delle funzioni e prodotto interno


### Derivata parziale, gradienti


### Gradiente e Machine Learning


### 